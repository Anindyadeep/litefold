{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anindya/esmfold-lite\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esmfold.model import ESMFold\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"esmfold_3B_v1\"\n",
    "url = f\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"\n",
    "model_data = torch.hub.load_state_dict_from_url(url, progress=True, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = model_data[\"cfg\"][\"model\"]\n",
    "model_state = model_data[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESMFold(esmfold_config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['esm.embed_tokens.weight', 'esm.layers.0.self_attn.k_proj.weight', 'esm.layers.0.self_attn.k_proj.bias', 'esm.layers.0.self_attn.v_proj.weight', 'esm.layers.0.self_attn.v_proj.bias', 'esm.layers.0.self_attn.q_proj.weight', 'esm.layers.0.self_attn.q_proj.bias', 'esm.layers.0.self_attn.out_proj.weight', 'esm.layers.0.self_attn.out_proj.bias', 'esm.layers.0.self_attn.rot_emb.inv_freq', 'esm.layers.0.self_attn_layer_norm.weight', 'esm.layers.0.self_attn_layer_norm.bias', 'esm.layers.0.fc1.weight', 'esm.layers.0.fc1.bias', 'esm.layers.0.fc2.weight', 'esm.layers.0.fc2.bias', 'esm.layers.0.final_layer_norm.weight', 'esm.layers.0.final_layer_norm.bias', 'esm.layers.1.self_attn.k_proj.weight', 'esm.layers.1.self_attn.k_proj.bias', 'esm.layers.1.self_attn.v_proj.weight', 'esm.layers.1.self_attn.v_proj.bias', 'esm.layers.1.self_attn.q_proj.weight', 'esm.layers.1.self_attn.q_proj.bias', 'esm.layers.1.self_attn.out_proj.weight', 'esm.layers.1.self_attn.out_proj.bias', 'esm.layers.1.self_attn.rot_emb.inv_freq', 'esm.layers.1.self_attn_layer_norm.weight', 'esm.layers.1.self_attn_layer_norm.bias', 'esm.layers.1.fc1.weight', 'esm.layers.1.fc1.bias', 'esm.layers.1.fc2.weight', 'esm.layers.1.fc2.bias', 'esm.layers.1.final_layer_norm.weight', 'esm.layers.1.final_layer_norm.bias', 'esm.layers.2.self_attn.k_proj.weight', 'esm.layers.2.self_attn.k_proj.bias', 'esm.layers.2.self_attn.v_proj.weight', 'esm.layers.2.self_attn.v_proj.bias', 'esm.layers.2.self_attn.q_proj.weight', 'esm.layers.2.self_attn.q_proj.bias', 'esm.layers.2.self_attn.out_proj.weight', 'esm.layers.2.self_attn.out_proj.bias', 'esm.layers.2.self_attn.rot_emb.inv_freq', 'esm.layers.2.self_attn_layer_norm.weight', 'esm.layers.2.self_attn_layer_norm.bias', 'esm.layers.2.fc1.weight', 'esm.layers.2.fc1.bias', 'esm.layers.2.fc2.weight', 'esm.layers.2.fc2.bias', 'esm.layers.2.final_layer_norm.weight', 'esm.layers.2.final_layer_norm.bias', 'esm.layers.3.self_attn.k_proj.weight', 'esm.layers.3.self_attn.k_proj.bias', 'esm.layers.3.self_attn.v_proj.weight', 'esm.layers.3.self_attn.v_proj.bias', 'esm.layers.3.self_attn.q_proj.weight', 'esm.layers.3.self_attn.q_proj.bias', 'esm.layers.3.self_attn.out_proj.weight', 'esm.layers.3.self_attn.out_proj.bias', 'esm.layers.3.self_attn.rot_emb.inv_freq', 'esm.layers.3.self_attn_layer_norm.weight', 'esm.layers.3.self_attn_layer_norm.bias', 'esm.layers.3.fc1.weight', 'esm.layers.3.fc1.bias', 'esm.layers.3.fc2.weight', 'esm.layers.3.fc2.bias', 'esm.layers.3.final_layer_norm.weight', 'esm.layers.3.final_layer_norm.bias', 'esm.layers.4.self_attn.k_proj.weight', 'esm.layers.4.self_attn.k_proj.bias', 'esm.layers.4.self_attn.v_proj.weight', 'esm.layers.4.self_attn.v_proj.bias', 'esm.layers.4.self_attn.q_proj.weight', 'esm.layers.4.self_attn.q_proj.bias', 'esm.layers.4.self_attn.out_proj.weight', 'esm.layers.4.self_attn.out_proj.bias', 'esm.layers.4.self_attn.rot_emb.inv_freq', 'esm.layers.4.self_attn_layer_norm.weight', 'esm.layers.4.self_attn_layer_norm.bias', 'esm.layers.4.fc1.weight', 'esm.layers.4.fc1.bias', 'esm.layers.4.fc2.weight', 'esm.layers.4.fc2.bias', 'esm.layers.4.final_layer_norm.weight', 'esm.layers.4.final_layer_norm.bias', 'esm.layers.5.self_attn.k_proj.weight', 'esm.layers.5.self_attn.k_proj.bias', 'esm.layers.5.self_attn.v_proj.weight', 'esm.layers.5.self_attn.v_proj.bias', 'esm.layers.5.self_attn.q_proj.weight', 'esm.layers.5.self_attn.q_proj.bias', 'esm.layers.5.self_attn.out_proj.weight', 'esm.layers.5.self_attn.out_proj.bias', 'esm.layers.5.self_attn.rot_emb.inv_freq', 'esm.layers.5.self_attn_layer_norm.weight', 'esm.layers.5.self_attn_layer_norm.bias', 'esm.layers.5.fc1.weight', 'esm.layers.5.fc1.bias', 'esm.layers.5.fc2.weight', 'esm.layers.5.fc2.bias', 'esm.layers.5.final_layer_norm.weight', 'esm.layers.5.final_layer_norm.bias', 'esm.layers.6.self_attn.k_proj.weight', 'esm.layers.6.self_attn.k_proj.bias', 'esm.layers.6.self_attn.v_proj.weight', 'esm.layers.6.self_attn.v_proj.bias', 'esm.layers.6.self_attn.q_proj.weight', 'esm.layers.6.self_attn.q_proj.bias', 'esm.layers.6.self_attn.out_proj.weight', 'esm.layers.6.self_attn.out_proj.bias', 'esm.layers.6.self_attn.rot_emb.inv_freq', 'esm.layers.6.self_attn_layer_norm.weight', 'esm.layers.6.self_attn_layer_norm.bias', 'esm.layers.6.fc1.weight', 'esm.layers.6.fc1.bias', 'esm.layers.6.fc2.weight', 'esm.layers.6.fc2.bias', 'esm.layers.6.final_layer_norm.weight', 'esm.layers.6.final_layer_norm.bias', 'esm.layers.7.self_attn.k_proj.weight', 'esm.layers.7.self_attn.k_proj.bias', 'esm.layers.7.self_attn.v_proj.weight', 'esm.layers.7.self_attn.v_proj.bias', 'esm.layers.7.self_attn.q_proj.weight', 'esm.layers.7.self_attn.q_proj.bias', 'esm.layers.7.self_attn.out_proj.weight', 'esm.layers.7.self_attn.out_proj.bias', 'esm.layers.7.self_attn.rot_emb.inv_freq', 'esm.layers.7.self_attn_layer_norm.weight', 'esm.layers.7.self_attn_layer_norm.bias', 'esm.layers.7.fc1.weight', 'esm.layers.7.fc1.bias', 'esm.layers.7.fc2.weight', 'esm.layers.7.fc2.bias', 'esm.layers.7.final_layer_norm.weight', 'esm.layers.7.final_layer_norm.bias', 'esm.layers.8.self_attn.k_proj.weight', 'esm.layers.8.self_attn.k_proj.bias', 'esm.layers.8.self_attn.v_proj.weight', 'esm.layers.8.self_attn.v_proj.bias', 'esm.layers.8.self_attn.q_proj.weight', 'esm.layers.8.self_attn.q_proj.bias', 'esm.layers.8.self_attn.out_proj.weight', 'esm.layers.8.self_attn.out_proj.bias', 'esm.layers.8.self_attn.rot_emb.inv_freq', 'esm.layers.8.self_attn_layer_norm.weight', 'esm.layers.8.self_attn_layer_norm.bias', 'esm.layers.8.fc1.weight', 'esm.layers.8.fc1.bias', 'esm.layers.8.fc2.weight', 'esm.layers.8.fc2.bias', 'esm.layers.8.final_layer_norm.weight', 'esm.layers.8.final_layer_norm.bias', 'esm.layers.9.self_attn.k_proj.weight', 'esm.layers.9.self_attn.k_proj.bias', 'esm.layers.9.self_attn.v_proj.weight', 'esm.layers.9.self_attn.v_proj.bias', 'esm.layers.9.self_attn.q_proj.weight', 'esm.layers.9.self_attn.q_proj.bias', 'esm.layers.9.self_attn.out_proj.weight', 'esm.layers.9.self_attn.out_proj.bias', 'esm.layers.9.self_attn.rot_emb.inv_freq', 'esm.layers.9.self_attn_layer_norm.weight', 'esm.layers.9.self_attn_layer_norm.bias', 'esm.layers.9.fc1.weight', 'esm.layers.9.fc1.bias', 'esm.layers.9.fc2.weight', 'esm.layers.9.fc2.bias', 'esm.layers.9.final_layer_norm.weight', 'esm.layers.9.final_layer_norm.bias', 'esm.layers.10.self_attn.k_proj.weight', 'esm.layers.10.self_attn.k_proj.bias', 'esm.layers.10.self_attn.v_proj.weight', 'esm.layers.10.self_attn.v_proj.bias', 'esm.layers.10.self_attn.q_proj.weight', 'esm.layers.10.self_attn.q_proj.bias', 'esm.layers.10.self_attn.out_proj.weight', 'esm.layers.10.self_attn.out_proj.bias', 'esm.layers.10.self_attn.rot_emb.inv_freq', 'esm.layers.10.self_attn_layer_norm.weight', 'esm.layers.10.self_attn_layer_norm.bias', 'esm.layers.10.fc1.weight', 'esm.layers.10.fc1.bias', 'esm.layers.10.fc2.weight', 'esm.layers.10.fc2.bias', 'esm.layers.10.final_layer_norm.weight', 'esm.layers.10.final_layer_norm.bias', 'esm.layers.11.self_attn.k_proj.weight', 'esm.layers.11.self_attn.k_proj.bias', 'esm.layers.11.self_attn.v_proj.weight', 'esm.layers.11.self_attn.v_proj.bias', 'esm.layers.11.self_attn.q_proj.weight', 'esm.layers.11.self_attn.q_proj.bias', 'esm.layers.11.self_attn.out_proj.weight', 'esm.layers.11.self_attn.out_proj.bias', 'esm.layers.11.self_attn.rot_emb.inv_freq', 'esm.layers.11.self_attn_layer_norm.weight', 'esm.layers.11.self_attn_layer_norm.bias', 'esm.layers.11.fc1.weight', 'esm.layers.11.fc1.bias', 'esm.layers.11.fc2.weight', 'esm.layers.11.fc2.bias', 'esm.layers.11.final_layer_norm.weight', 'esm.layers.11.final_layer_norm.bias', 'esm.layers.12.self_attn.k_proj.weight', 'esm.layers.12.self_attn.k_proj.bias', 'esm.layers.12.self_attn.v_proj.weight', 'esm.layers.12.self_attn.v_proj.bias', 'esm.layers.12.self_attn.q_proj.weight', 'esm.layers.12.self_attn.q_proj.bias', 'esm.layers.12.self_attn.out_proj.weight', 'esm.layers.12.self_attn.out_proj.bias', 'esm.layers.12.self_attn.rot_emb.inv_freq', 'esm.layers.12.self_attn_layer_norm.weight', 'esm.layers.12.self_attn_layer_norm.bias', 'esm.layers.12.fc1.weight', 'esm.layers.12.fc1.bias', 'esm.layers.12.fc2.weight', 'esm.layers.12.fc2.bias', 'esm.layers.12.final_layer_norm.weight', 'esm.layers.12.final_layer_norm.bias', 'esm.layers.13.self_attn.k_proj.weight', 'esm.layers.13.self_attn.k_proj.bias', 'esm.layers.13.self_attn.v_proj.weight', 'esm.layers.13.self_attn.v_proj.bias', 'esm.layers.13.self_attn.q_proj.weight', 'esm.layers.13.self_attn.q_proj.bias', 'esm.layers.13.self_attn.out_proj.weight', 'esm.layers.13.self_attn.out_proj.bias', 'esm.layers.13.self_attn.rot_emb.inv_freq', 'esm.layers.13.self_attn_layer_norm.weight', 'esm.layers.13.self_attn_layer_norm.bias', 'esm.layers.13.fc1.weight', 'esm.layers.13.fc1.bias', 'esm.layers.13.fc2.weight', 'esm.layers.13.fc2.bias', 'esm.layers.13.final_layer_norm.weight', 'esm.layers.13.final_layer_norm.bias', 'esm.layers.14.self_attn.k_proj.weight', 'esm.layers.14.self_attn.k_proj.bias', 'esm.layers.14.self_attn.v_proj.weight', 'esm.layers.14.self_attn.v_proj.bias', 'esm.layers.14.self_attn.q_proj.weight', 'esm.layers.14.self_attn.q_proj.bias', 'esm.layers.14.self_attn.out_proj.weight', 'esm.layers.14.self_attn.out_proj.bias', 'esm.layers.14.self_attn.rot_emb.inv_freq', 'esm.layers.14.self_attn_layer_norm.weight', 'esm.layers.14.self_attn_layer_norm.bias', 'esm.layers.14.fc1.weight', 'esm.layers.14.fc1.bias', 'esm.layers.14.fc2.weight', 'esm.layers.14.fc2.bias', 'esm.layers.14.final_layer_norm.weight', 'esm.layers.14.final_layer_norm.bias', 'esm.layers.15.self_attn.k_proj.weight', 'esm.layers.15.self_attn.k_proj.bias', 'esm.layers.15.self_attn.v_proj.weight', 'esm.layers.15.self_attn.v_proj.bias', 'esm.layers.15.self_attn.q_proj.weight', 'esm.layers.15.self_attn.q_proj.bias', 'esm.layers.15.self_attn.out_proj.weight', 'esm.layers.15.self_attn.out_proj.bias', 'esm.layers.15.self_attn.rot_emb.inv_freq', 'esm.layers.15.self_attn_layer_norm.weight', 'esm.layers.15.self_attn_layer_norm.bias', 'esm.layers.15.fc1.weight', 'esm.layers.15.fc1.bias', 'esm.layers.15.fc2.weight', 'esm.layers.15.fc2.bias', 'esm.layers.15.final_layer_norm.weight', 'esm.layers.15.final_layer_norm.bias', 'esm.layers.16.self_attn.k_proj.weight', 'esm.layers.16.self_attn.k_proj.bias', 'esm.layers.16.self_attn.v_proj.weight', 'esm.layers.16.self_attn.v_proj.bias', 'esm.layers.16.self_attn.q_proj.weight', 'esm.layers.16.self_attn.q_proj.bias', 'esm.layers.16.self_attn.out_proj.weight', 'esm.layers.16.self_attn.out_proj.bias', 'esm.layers.16.self_attn.rot_emb.inv_freq', 'esm.layers.16.self_attn_layer_norm.weight', 'esm.layers.16.self_attn_layer_norm.bias', 'esm.layers.16.fc1.weight', 'esm.layers.16.fc1.bias', 'esm.layers.16.fc2.weight', 'esm.layers.16.fc2.bias', 'esm.layers.16.final_layer_norm.weight', 'esm.layers.16.final_layer_norm.bias', 'esm.layers.17.self_attn.k_proj.weight', 'esm.layers.17.self_attn.k_proj.bias', 'esm.layers.17.self_attn.v_proj.weight', 'esm.layers.17.self_attn.v_proj.bias', 'esm.layers.17.self_attn.q_proj.weight', 'esm.layers.17.self_attn.q_proj.bias', 'esm.layers.17.self_attn.out_proj.weight', 'esm.layers.17.self_attn.out_proj.bias', 'esm.layers.17.self_attn.rot_emb.inv_freq', 'esm.layers.17.self_attn_layer_norm.weight', 'esm.layers.17.self_attn_layer_norm.bias', 'esm.layers.17.fc1.weight', 'esm.layers.17.fc1.bias', 'esm.layers.17.fc2.weight', 'esm.layers.17.fc2.bias', 'esm.layers.17.final_layer_norm.weight', 'esm.layers.17.final_layer_norm.bias', 'esm.layers.18.self_attn.k_proj.weight', 'esm.layers.18.self_attn.k_proj.bias', 'esm.layers.18.self_attn.v_proj.weight', 'esm.layers.18.self_attn.v_proj.bias', 'esm.layers.18.self_attn.q_proj.weight', 'esm.layers.18.self_attn.q_proj.bias', 'esm.layers.18.self_attn.out_proj.weight', 'esm.layers.18.self_attn.out_proj.bias', 'esm.layers.18.self_attn.rot_emb.inv_freq', 'esm.layers.18.self_attn_layer_norm.weight', 'esm.layers.18.self_attn_layer_norm.bias', 'esm.layers.18.fc1.weight', 'esm.layers.18.fc1.bias', 'esm.layers.18.fc2.weight', 'esm.layers.18.fc2.bias', 'esm.layers.18.final_layer_norm.weight', 'esm.layers.18.final_layer_norm.bias', 'esm.layers.19.self_attn.k_proj.weight', 'esm.layers.19.self_attn.k_proj.bias', 'esm.layers.19.self_attn.v_proj.weight', 'esm.layers.19.self_attn.v_proj.bias', 'esm.layers.19.self_attn.q_proj.weight', 'esm.layers.19.self_attn.q_proj.bias', 'esm.layers.19.self_attn.out_proj.weight', 'esm.layers.19.self_attn.out_proj.bias', 'esm.layers.19.self_attn.rot_emb.inv_freq', 'esm.layers.19.self_attn_layer_norm.weight', 'esm.layers.19.self_attn_layer_norm.bias', 'esm.layers.19.fc1.weight', 'esm.layers.19.fc1.bias', 'esm.layers.19.fc2.weight', 'esm.layers.19.fc2.bias', 'esm.layers.19.final_layer_norm.weight', 'esm.layers.19.final_layer_norm.bias', 'esm.layers.20.self_attn.k_proj.weight', 'esm.layers.20.self_attn.k_proj.bias', 'esm.layers.20.self_attn.v_proj.weight', 'esm.layers.20.self_attn.v_proj.bias', 'esm.layers.20.self_attn.q_proj.weight', 'esm.layers.20.self_attn.q_proj.bias', 'esm.layers.20.self_attn.out_proj.weight', 'esm.layers.20.self_attn.out_proj.bias', 'esm.layers.20.self_attn.rot_emb.inv_freq', 'esm.layers.20.self_attn_layer_norm.weight', 'esm.layers.20.self_attn_layer_norm.bias', 'esm.layers.20.fc1.weight', 'esm.layers.20.fc1.bias', 'esm.layers.20.fc2.weight', 'esm.layers.20.fc2.bias', 'esm.layers.20.final_layer_norm.weight', 'esm.layers.20.final_layer_norm.bias', 'esm.layers.21.self_attn.k_proj.weight', 'esm.layers.21.self_attn.k_proj.bias', 'esm.layers.21.self_attn.v_proj.weight', 'esm.layers.21.self_attn.v_proj.bias', 'esm.layers.21.self_attn.q_proj.weight', 'esm.layers.21.self_attn.q_proj.bias', 'esm.layers.21.self_attn.out_proj.weight', 'esm.layers.21.self_attn.out_proj.bias', 'esm.layers.21.self_attn.rot_emb.inv_freq', 'esm.layers.21.self_attn_layer_norm.weight', 'esm.layers.21.self_attn_layer_norm.bias', 'esm.layers.21.fc1.weight', 'esm.layers.21.fc1.bias', 'esm.layers.21.fc2.weight', 'esm.layers.21.fc2.bias', 'esm.layers.21.final_layer_norm.weight', 'esm.layers.21.final_layer_norm.bias', 'esm.layers.22.self_attn.k_proj.weight', 'esm.layers.22.self_attn.k_proj.bias', 'esm.layers.22.self_attn.v_proj.weight', 'esm.layers.22.self_attn.v_proj.bias', 'esm.layers.22.self_attn.q_proj.weight', 'esm.layers.22.self_attn.q_proj.bias', 'esm.layers.22.self_attn.out_proj.weight', 'esm.layers.22.self_attn.out_proj.bias', 'esm.layers.22.self_attn.rot_emb.inv_freq', 'esm.layers.22.self_attn_layer_norm.weight', 'esm.layers.22.self_attn_layer_norm.bias', 'esm.layers.22.fc1.weight', 'esm.layers.22.fc1.bias', 'esm.layers.22.fc2.weight', 'esm.layers.22.fc2.bias', 'esm.layers.22.final_layer_norm.weight', 'esm.layers.22.final_layer_norm.bias', 'esm.layers.23.self_attn.k_proj.weight', 'esm.layers.23.self_attn.k_proj.bias', 'esm.layers.23.self_attn.v_proj.weight', 'esm.layers.23.self_attn.v_proj.bias', 'esm.layers.23.self_attn.q_proj.weight', 'esm.layers.23.self_attn.q_proj.bias', 'esm.layers.23.self_attn.out_proj.weight', 'esm.layers.23.self_attn.out_proj.bias', 'esm.layers.23.self_attn.rot_emb.inv_freq', 'esm.layers.23.self_attn_layer_norm.weight', 'esm.layers.23.self_attn_layer_norm.bias', 'esm.layers.23.fc1.weight', 'esm.layers.23.fc1.bias', 'esm.layers.23.fc2.weight', 'esm.layers.23.fc2.bias', 'esm.layers.23.final_layer_norm.weight', 'esm.layers.23.final_layer_norm.bias', 'esm.layers.24.self_attn.k_proj.weight', 'esm.layers.24.self_attn.k_proj.bias', 'esm.layers.24.self_attn.v_proj.weight', 'esm.layers.24.self_attn.v_proj.bias', 'esm.layers.24.self_attn.q_proj.weight', 'esm.layers.24.self_attn.q_proj.bias', 'esm.layers.24.self_attn.out_proj.weight', 'esm.layers.24.self_attn.out_proj.bias', 'esm.layers.24.self_attn.rot_emb.inv_freq', 'esm.layers.24.self_attn_layer_norm.weight', 'esm.layers.24.self_attn_layer_norm.bias', 'esm.layers.24.fc1.weight', 'esm.layers.24.fc1.bias', 'esm.layers.24.fc2.weight', 'esm.layers.24.fc2.bias', 'esm.layers.24.final_layer_norm.weight', 'esm.layers.24.final_layer_norm.bias', 'esm.layers.25.self_attn.k_proj.weight', 'esm.layers.25.self_attn.k_proj.bias', 'esm.layers.25.self_attn.v_proj.weight', 'esm.layers.25.self_attn.v_proj.bias', 'esm.layers.25.self_attn.q_proj.weight', 'esm.layers.25.self_attn.q_proj.bias', 'esm.layers.25.self_attn.out_proj.weight', 'esm.layers.25.self_attn.out_proj.bias', 'esm.layers.25.self_attn.rot_emb.inv_freq', 'esm.layers.25.self_attn_layer_norm.weight', 'esm.layers.25.self_attn_layer_norm.bias', 'esm.layers.25.fc1.weight', 'esm.layers.25.fc1.bias', 'esm.layers.25.fc2.weight', 'esm.layers.25.fc2.bias', 'esm.layers.25.final_layer_norm.weight', 'esm.layers.25.final_layer_norm.bias', 'esm.layers.26.self_attn.k_proj.weight', 'esm.layers.26.self_attn.k_proj.bias', 'esm.layers.26.self_attn.v_proj.weight', 'esm.layers.26.self_attn.v_proj.bias', 'esm.layers.26.self_attn.q_proj.weight', 'esm.layers.26.self_attn.q_proj.bias', 'esm.layers.26.self_attn.out_proj.weight', 'esm.layers.26.self_attn.out_proj.bias', 'esm.layers.26.self_attn.rot_emb.inv_freq', 'esm.layers.26.self_attn_layer_norm.weight', 'esm.layers.26.self_attn_layer_norm.bias', 'esm.layers.26.fc1.weight', 'esm.layers.26.fc1.bias', 'esm.layers.26.fc2.weight', 'esm.layers.26.fc2.bias', 'esm.layers.26.final_layer_norm.weight', 'esm.layers.26.final_layer_norm.bias', 'esm.layers.27.self_attn.k_proj.weight', 'esm.layers.27.self_attn.k_proj.bias', 'esm.layers.27.self_attn.v_proj.weight', 'esm.layers.27.self_attn.v_proj.bias', 'esm.layers.27.self_attn.q_proj.weight', 'esm.layers.27.self_attn.q_proj.bias', 'esm.layers.27.self_attn.out_proj.weight', 'esm.layers.27.self_attn.out_proj.bias', 'esm.layers.27.self_attn.rot_emb.inv_freq', 'esm.layers.27.self_attn_layer_norm.weight', 'esm.layers.27.self_attn_layer_norm.bias', 'esm.layers.27.fc1.weight', 'esm.layers.27.fc1.bias', 'esm.layers.27.fc2.weight', 'esm.layers.27.fc2.bias', 'esm.layers.27.final_layer_norm.weight', 'esm.layers.27.final_layer_norm.bias', 'esm.layers.28.self_attn.k_proj.weight', 'esm.layers.28.self_attn.k_proj.bias', 'esm.layers.28.self_attn.v_proj.weight', 'esm.layers.28.self_attn.v_proj.bias', 'esm.layers.28.self_attn.q_proj.weight', 'esm.layers.28.self_attn.q_proj.bias', 'esm.layers.28.self_attn.out_proj.weight', 'esm.layers.28.self_attn.out_proj.bias', 'esm.layers.28.self_attn.rot_emb.inv_freq', 'esm.layers.28.self_attn_layer_norm.weight', 'esm.layers.28.self_attn_layer_norm.bias', 'esm.layers.28.fc1.weight', 'esm.layers.28.fc1.bias', 'esm.layers.28.fc2.weight', 'esm.layers.28.fc2.bias', 'esm.layers.28.final_layer_norm.weight', 'esm.layers.28.final_layer_norm.bias', 'esm.layers.29.self_attn.k_proj.weight', 'esm.layers.29.self_attn.k_proj.bias', 'esm.layers.29.self_attn.v_proj.weight', 'esm.layers.29.self_attn.v_proj.bias', 'esm.layers.29.self_attn.q_proj.weight', 'esm.layers.29.self_attn.q_proj.bias', 'esm.layers.29.self_attn.out_proj.weight', 'esm.layers.29.self_attn.out_proj.bias', 'esm.layers.29.self_attn.rot_emb.inv_freq', 'esm.layers.29.self_attn_layer_norm.weight', 'esm.layers.29.self_attn_layer_norm.bias', 'esm.layers.29.fc1.weight', 'esm.layers.29.fc1.bias', 'esm.layers.29.fc2.weight', 'esm.layers.29.fc2.bias', 'esm.layers.29.final_layer_norm.weight', 'esm.layers.29.final_layer_norm.bias', 'esm.layers.30.self_attn.k_proj.weight', 'esm.layers.30.self_attn.k_proj.bias', 'esm.layers.30.self_attn.v_proj.weight', 'esm.layers.30.self_attn.v_proj.bias', 'esm.layers.30.self_attn.q_proj.weight', 'esm.layers.30.self_attn.q_proj.bias', 'esm.layers.30.self_attn.out_proj.weight', 'esm.layers.30.self_attn.out_proj.bias', 'esm.layers.30.self_attn.rot_emb.inv_freq', 'esm.layers.30.self_attn_layer_norm.weight', 'esm.layers.30.self_attn_layer_norm.bias', 'esm.layers.30.fc1.weight', 'esm.layers.30.fc1.bias', 'esm.layers.30.fc2.weight', 'esm.layers.30.fc2.bias', 'esm.layers.30.final_layer_norm.weight', 'esm.layers.30.final_layer_norm.bias', 'esm.layers.31.self_attn.k_proj.weight', 'esm.layers.31.self_attn.k_proj.bias', 'esm.layers.31.self_attn.v_proj.weight', 'esm.layers.31.self_attn.v_proj.bias', 'esm.layers.31.self_attn.q_proj.weight', 'esm.layers.31.self_attn.q_proj.bias', 'esm.layers.31.self_attn.out_proj.weight', 'esm.layers.31.self_attn.out_proj.bias', 'esm.layers.31.self_attn.rot_emb.inv_freq', 'esm.layers.31.self_attn_layer_norm.weight', 'esm.layers.31.self_attn_layer_norm.bias', 'esm.layers.31.fc1.weight', 'esm.layers.31.fc1.bias', 'esm.layers.31.fc2.weight', 'esm.layers.31.fc2.bias', 'esm.layers.31.final_layer_norm.weight', 'esm.layers.31.final_layer_norm.bias', 'esm.layers.32.self_attn.k_proj.weight', 'esm.layers.32.self_attn.k_proj.bias', 'esm.layers.32.self_attn.v_proj.weight', 'esm.layers.32.self_attn.v_proj.bias', 'esm.layers.32.self_attn.q_proj.weight', 'esm.layers.32.self_attn.q_proj.bias', 'esm.layers.32.self_attn.out_proj.weight', 'esm.layers.32.self_attn.out_proj.bias', 'esm.layers.32.self_attn.rot_emb.inv_freq', 'esm.layers.32.self_attn_layer_norm.weight', 'esm.layers.32.self_attn_layer_norm.bias', 'esm.layers.32.fc1.weight', 'esm.layers.32.fc1.bias', 'esm.layers.32.fc2.weight', 'esm.layers.32.fc2.bias', 'esm.layers.32.final_layer_norm.weight', 'esm.layers.32.final_layer_norm.bias', 'esm.layers.33.self_attn.k_proj.weight', 'esm.layers.33.self_attn.k_proj.bias', 'esm.layers.33.self_attn.v_proj.weight', 'esm.layers.33.self_attn.v_proj.bias', 'esm.layers.33.self_attn.q_proj.weight', 'esm.layers.33.self_attn.q_proj.bias', 'esm.layers.33.self_attn.out_proj.weight', 'esm.layers.33.self_attn.out_proj.bias', 'esm.layers.33.self_attn.rot_emb.inv_freq', 'esm.layers.33.self_attn_layer_norm.weight', 'esm.layers.33.self_attn_layer_norm.bias', 'esm.layers.33.fc1.weight', 'esm.layers.33.fc1.bias', 'esm.layers.33.fc2.weight', 'esm.layers.33.fc2.bias', 'esm.layers.33.final_layer_norm.weight', 'esm.layers.33.final_layer_norm.bias', 'esm.layers.34.self_attn.k_proj.weight', 'esm.layers.34.self_attn.k_proj.bias', 'esm.layers.34.self_attn.v_proj.weight', 'esm.layers.34.self_attn.v_proj.bias', 'esm.layers.34.self_attn.q_proj.weight', 'esm.layers.34.self_attn.q_proj.bias', 'esm.layers.34.self_attn.out_proj.weight', 'esm.layers.34.self_attn.out_proj.bias', 'esm.layers.34.self_attn.rot_emb.inv_freq', 'esm.layers.34.self_attn_layer_norm.weight', 'esm.layers.34.self_attn_layer_norm.bias', 'esm.layers.34.fc1.weight', 'esm.layers.34.fc1.bias', 'esm.layers.34.fc2.weight', 'esm.layers.34.fc2.bias', 'esm.layers.34.final_layer_norm.weight', 'esm.layers.34.final_layer_norm.bias', 'esm.layers.35.self_attn.k_proj.weight', 'esm.layers.35.self_attn.k_proj.bias', 'esm.layers.35.self_attn.v_proj.weight', 'esm.layers.35.self_attn.v_proj.bias', 'esm.layers.35.self_attn.q_proj.weight', 'esm.layers.35.self_attn.q_proj.bias', 'esm.layers.35.self_attn.out_proj.weight', 'esm.layers.35.self_attn.out_proj.bias', 'esm.layers.35.self_attn.rot_emb.inv_freq', 'esm.layers.35.self_attn_layer_norm.weight', 'esm.layers.35.self_attn_layer_norm.bias', 'esm.layers.35.fc1.weight', 'esm.layers.35.fc1.bias', 'esm.layers.35.fc2.weight', 'esm.layers.35.fc2.bias', 'esm.layers.35.final_layer_norm.weight', 'esm.layers.35.final_layer_norm.bias', 'esm.contact_head.regression.weight', 'esm.contact_head.regression.bias', 'esm.emb_layer_norm_after.weight', 'esm.emb_layer_norm_after.bias', 'esm.lm_head.weight', 'esm.lm_head.bias', 'esm.lm_head.dense.weight', 'esm.lm_head.dense.bias', 'esm.lm_head.layer_norm.weight', 'esm.lm_head.layer_norm.bias', 'trunk.structure_module.ipa.linear_q_points.linear.weight', 'trunk.structure_module.ipa.linear_q_points.linear.bias', 'trunk.structure_module.ipa.linear_kv_points.linear.weight', 'trunk.structure_module.ipa.linear_kv_points.linear.bias'], unexpected_keys=['positional_encoding._float_tensor', 'trunk.structure_module.default_frames', 'trunk.structure_module.group_idx', 'trunk.structure_module.atom_mask', 'trunk.structure_module.lit_positions', 'trunk.structure_module.ipa.linear_q_points.weight', 'trunk.structure_module.ipa.linear_q_points.bias', 'trunk.structure_module.ipa.linear_kv_points.weight', 'trunk.structure_module.ipa.linear_kv_points.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_keys = set(model.state_dict().keys())\n",
    "found_keys = set(model_state.keys())\n",
    "\n",
    "missing_essential_keys = []\n",
    "for missing_key in expected_keys - found_keys:\n",
    "    if not missing_key.startswith(\"esm.\"):\n",
    "        missing_essential_keys.append(missing_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trunk.structure_module.ipa.linear_kv_points.linear.weight',\n",
       " 'trunk.structure_module.ipa.linear_q_points.linear.bias',\n",
       " 'trunk.structure_module.ipa.linear_q_points.linear.weight',\n",
       " 'trunk.structure_module.ipa.linear_kv_points.linear.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_essential_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().to(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n",
    "# Multimer prediction can be done with chains separated by ':'\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.infer_pdb(sequence)\n",
    "\n",
    "with open(\"result.pdb\", \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biotite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biotite.structure.io as bsio\n",
    "struct = bsio.load_structure(\"result.pdb\", extra_fields=[\"b_factor\"])\n",
    "print(struct.b_factor.mean())  # this will be the pLDDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]\n",
    "\n",
    "# Generate per-sequence representations via averaging\n",
    "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "sequence_representations = []\n",
    "for i, tokens_len in enumerate(batch_lens):\n",
    "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "# Look at the unsupervised self-attention map contact predictions\n",
    "import matplotlib.pyplot as plt\n",
    "for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n",
    "    plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
    "    plt.title(seq)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from esmfold.model import ESMFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = esm.pretrained.esmfold_v1()\n",
    "model = model.eval().cuda()\n",
    "\n",
    "# Optionally, uncomment to set a chunk size for axial attention. This can help reduce memory.\n",
    "# Lower sizes will have lower memory requirements at the cost of increased speed.\n",
    "# model.set_chunk_size(128)\n",
    "\n",
    "sequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n",
    "# Multimer prediction can be done with chains separated by ':'\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.infer_pdb(sequence)\n",
    "\n",
    "with open(\"result.pdb\", \"w\") as f:\n",
    "    f.write(output)\n",
    "\n",
    "import biotite.structure.io as bsio\n",
    "struct = bsio.load_structure(\"result.pdb\", extra_fields=[\"b_factor\"])\n",
    "print(struct.b_factor.mean())  # this will be the pLDDT\n",
    "# 88.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
